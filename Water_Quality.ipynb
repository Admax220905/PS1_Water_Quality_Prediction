{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "from pathlib import Path\n",
    "# Optional: For Chart.js PNG export\n",
    "from playwright.sync_api import sync_playwright\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = 'output'\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(f'Output directory created/verified at: {os.path.abspath(output_dir)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "try:\n",
    "    data = pd.read_csv('Data.csv')\n",
    "    if 'Unnamed: 0' in data.columns:\n",
    "        data = data.drop(columns=['Unnamed: 0'])\n",
    "    data = data.dropna()\n",
    "    print('Data loaded successfully.')\n",
    "    print('Missing Values:\\n', data.isnull().sum())\n",
    "    print('\\nClass Distribution:\\n', data['Label'].value_counts(normalize=True))\n",
    "    X = data[['Chloride', 'Organic_Carbon', 'Solids', 'Sulphate', 'Turbidity', 'ph']]\n",
    "    y = data['Label']\n",
    "except FileNotFoundError:\n",
    "    print('Error: Data.csv not found. Please ensure the file exists in the working directory.')\n",
    "except KeyError as e:\n",
    "    print(f'Error: Column {e} not found in Data.csv. Required columns: Chloride, Organic_Carbon, Solids, Sulphate, Turbidity, ph, Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and scale data\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    joblib.dump(scaler, os.path.join(output_dir, 'scaler.pkl'))\n",
    "    print('Data split and scaled successfully.')\n",
    "except NameError:\n",
    "    print('Error: X or y not defined. Ensure the previous cell ran successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100, class_weight='balanced'),\n",
    "    'SVM': SVC(random_state=42, probability=True, class_weight='balanced'),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        print(f'{name} trained successfully.')\n",
    "    except Exception as e:\n",
    "        print(f'Error training {name}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        results[name] = {\n",
    "            'Precision': precision_score(y_test, y_pred),\n",
    "            'Recall': recall_score(y_test, y_pred),\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'F1-Score': f1_score(y_test, y_pred),\n",
    "            'Confusion Matrix': cm.tolist()\n",
    "        }\n",
    "        \n",
    "        print(f'\\n{name}:')\n",
    "        for metric, value in results[name].items():\n",
    "            if metric != 'Confusion Matrix':\n",
    "                print(f'{metric}: {value:.4f}')\n",
    "    except Exception as e:\n",
    "        print(f'Error evaluating {name}: {e}')\n",
    "\n",
    "# Print results for debugging\n",
    "print('\\nResults dictionary:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "try:\n",
    "    best_model_name = max(results, key=lambda x: results[x]['F1-Score'])\n",
    "    joblib.dump(models[best_model_name], os.path.join(output_dir, f'best_model_{best_model_name.replace(\" \", \"_\")}.pkl'))\n",
    "    print(f'\\nBest model ({best_model_name}) saved')\n",
    "except ValueError:\n",
    "    print('Error: No models evaluated. Ensure the previous cell ran successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization (Matplotlib/Seaborn)\n",
    "try:\n",
    "    if not results:\n",
    "        raise ValueError('Results dictionary is empty. Ensure models were evaluated successfully.')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Model': [name for name in results.keys() for _ in range(4)],\n",
    "        'Metric': ['Precision', 'Recall', 'Accuracy', 'F1-Score'] * len(results),\n",
    "        'Value': [results[name][metric] for name in results for metric in ['Precision', 'Recall', 'Accuracy', 'F1-Score']]\n",
    "    })\n",
    "    sns.barplot(x='Metric', y='Value', hue='Model', data=metrics_df)\n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.ylim(0, 1)\n",
    "    output_path = os.path.join(output_dir, 'model_comparison.png')\n",
    "    plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    print(f'Plot saved successfully as {output_path}')\n",
    "except Exception as e:\n",
    "    print(f'Error generating Matplotlib plot: {e}')\n",
    "\n",
    "# Optional: Chart.js Visualization and PNG Export\n",
    "try:\n",
    "    chart_config = {\n",
    "        'type': 'bar',\n",
    "        'data': {\n",
    "            'labels': ['Precision', 'Recall', 'Accuracy', 'F1-Score'],\n",
    "            'datasets': [\n",
    "                {\n",
    "                    'label': name,\n",
    "                    'data': [results[name][metric] for metric in ['Precision', 'Recall', 'Accuracy', 'F1-Score']],\n",
    "                    'backgroundColor': f'rgba({(i*50 % 255)}, {(i*100 % 255)}, {(i*150 % 255)}, 0.6)',\n",
    "                    'borderColor': f'rgba({(i*50 % 255)}, {(i*100 % 255)}, {(i*150 % 255)}, 1)',\n",
    "                    'borderWidth': 1\n",
    "                } for i, name in enumerate(results.keys())\n",
    "            ]\n",
    "        },\n",
    "        'options': {\n",
    "            'plugins': {\n",
    "                'title': {'display': True, 'text': 'Model Performance Comparison', 'font': {'size': 18}},\n",
    "                'legend': {'position': 'top'}\n",
    "            },\n",
    "            'scales': {\n",
    "                'y': {'beginAtZero': True, 'max': 1, 'title': {'display': True, 'text': 'Value'}},\n",
    "                'x': {'title': {'display': True, 'text': 'Metric'}}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    # Save Chart.js config\n",
    "    chart_config_path = os.path.join(output_dir, 'chart_config.json')\n",
    "    with open(chart_config_path, 'w') as f:\n",
    "        json.dump(chart_config, f)\n",
    "\n",
    "    # HTML to render Chart.js chart\n",
    "    html_content = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <script src='https://cdn.jsdelivr.net/npm/chart.js'></script>\n",
    "    </head>\n",
    "    <body>\n",
    "        <canvas id='myChart' width='800' height='400'></canvas>\n",
    "        <script>\n",
    "            fetch('{chart_config_path}')\n",
    "                .then(response => response.json())\n",
    "                .then(config => new Chart(document.getElementById('myChart'), config));\n",
    "        </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    html_path = os.path.join(output_dir, 'chart.html')\n",
    "    with open(html_path, 'w') as f:\n",
    "        f.write(html_content)\n",
    "\n",
    "    # Use Playwright to save PNG\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch()\n",
    "        page = browser.new_page()\n",
    "        page.goto(f'file://{os.path.abspath(html_path)}')\n",
    "        page.wait_for_timeout(1000)\n",
    "        chart_png_path = os.path.join(output_dir, 'model_comparison_chartjs.png')\n",
    "        page.screenshot(path=chart_png_path)\n",
    "        browser.close()\n",
    "    print(f'Chart.js chart saved as {chart_png_path}')\n",
    "except Exception as e:\n",
    "    print(f'Error generating Chart.js chart: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
