import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import (precision_score, recall_score, 
                             accuracy_score, f1_score, confusion_matrix)
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import os

# Create output directory
os.makedirs('output', exist_ok=True)

# 1. Data Loading and Preparation
def load_data(path='Data.csv'):
    try:
        data = pd.read_csv(path)
    except Exception as e:
        print(f"Failed to load data: {e}")
        return None
    if 'Unnamed: 0' in data.columns:
        data = data.drop(columns=['Unnamed: 0'])
    data = data.dropna()
    return data

# 2. Data Analysis
def analyze_data(data):
    print("Missing Values:\n", data.isnull().sum())
    print("\nClass Distribution:\n", data['Label'].value_counts(normalize=True))
    X = data[['Chloride', 'Organic_Carbon', 'Solids', 'Sulphate', 'Turbidity', 'ph']]
    y = data['Label']
    return X, y

# 3. Model Training
def train_models(X_train, y_train):
    models = {
        'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),
        'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100, class_weight='balanced'),
        'SVM': SVC(random_state=42, probability=True, class_weight='balanced'),
        'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)
    }
    
    results = {}
    for name, model in models.items():
        model.fit(X_train, y_train)
        results[name] = {'Model': model}
        print(f"{name} trained successfully.")
    return models, results

# 4. Model Evaluation
def evaluate_models(models, X_test, y_test):
    results = {}
    for name, model in models.items():
        y_pred = model.predict(X_test)
        cm = confusion_matrix(y_test, y_pred)
        # Handle both binary and multiclass confusion matrices
        if cm.shape == (2,2):
            tn, fp, fn, tp = cm.ravel()
            specificity = tn / (tn + fp) if (tn + fp) > 0 else np.nan
        else:
            specificity = np.nan  # Not defined for multiclass
        results[name] = {
            'Precision': precision_score(y_test, y_pred, average='binary', zero_division=0),
            'Recall': recall_score(y_test, y_pred, average='binary', zero_division=0),
            'Accuracy': accuracy_score(y_test, y_pred),
            'F1-Score': f1_score(y_test, y_pred, average='binary', zero_division=0),
            'Sensitivity': recall_score(y_test, y_pred, average='binary', zero_division=0),
            'Specificity': specificity,
            'Confusion Matrix': cm,
            'Model': model
        }
    return results

# 5. Visualization
def visualize_results(results, X_columns):
    # Confusion Matrices
    n_models = len(results)
    n_rows = int(np.ceil(n_models / 2))
    fig, axes = plt.subplots(n_rows, 2, figsize=(12, 5 * n_rows))
    axes = axes.ravel() if n_models > 1 else [axes]

    for idx, (name, metrics) in enumerate(results.items()):
        ax = axes[idx]
        sns.heatmap(metrics['Confusion Matrix'], annot=True, fmt='d', 
                    cmap='Blues', ax=ax, cbar=False)
        ax.set_title(name)
    for idx in range(len(results), len(axes)):
        fig.delaxes(axes[idx])
    plt.tight_layout()
    plt.savefig('output/confusion_matrices.png', dpi=300)
    plt.close()
    
    # Feature Importance (Random Forest only)
    if 'Random Forest' in results:
        rf_model = results['Random Forest']['Model']
        importances = pd.Series(rf_model.feature_importances_, index=X_columns)
        plt.figure(figsize=(10, 6))
        importances.sort_values().plot(kind='barh', color='skyblue')
        plt.title('Feature Importance (Random Forest)')
        plt.savefig('output/feature_importance.png', bbox_inches='tight')
        plt.close()

# Main Execution (as a notebook, run in sequence)
data = load_data()
if data is not None:
    X, y = analyze_data(data)

    # Split and scale data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y)

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    joblib.dump(scaler, 'output/scaler.pkl')

    # Train and evaluate models
    models, _ = train_models(X_train_scaled, y_train)
    results = evaluate_models(models, X_test_scaled, y_test)

    # Save best model
    best_model_name = max(results, key=lambda x: results[x]['F1-Score'])
    joblib.dump(results[best_model_name]['Model'], 
                f'output/best_model_{best_model_name.replace(" ", "_")}.pkl')

    # Generate visualizations
    visualize_results(results, X.columns)

    # Print results
    print("\nFinal Metrics:")
    for name, metrics in results.items():
        print(f"\n{name}:")
        for metric, value in metrics.items():
            if metric not in ['Model', 'Confusion Matrix']:
                print(f"{metric}: {value:.4f}")

    print("Execution completed successfully!")
else:
    print("Data loading failed. Please check the CSV file and path.")
