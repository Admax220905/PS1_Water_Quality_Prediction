{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import display, Image\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = 'output'\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(f'Output directory created/verified at: {os.path.abspath(output_dir)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "try:\n",
    "    data = pd.read_csv('Data.csv')\n",
    "    if 'Unnamed: 0' in data.columns:\n",
    "        data = data.drop(columns=['Unnamed: 0'])\n",
    "    data = data.dropna()\n",
    "    print('Data loaded successfully.')\n",
    "    print('Missing Values:\\n', data.isnull().sum())\n",
    "    print('\\nClass Distribution:\\n', data['Label'].value_counts(normalize=True))\n",
    "    X = data[['Chloride', 'Organic_Carbon', 'Solids', 'Sulphate', 'Turbidity', 'ph']]\n",
    "    y = data['Label']\n",
    "except FileNotFoundError:\n",
    "    print('Error: Data.csv not found. Please ensure the file exists in the working directory.')\n",
    "except KeyError as e:\n",
    "    print(f'Error: Column {e} not found in Data.csv. Required columns: Chloride, Organic_Carbon, Solids, Sulphate, Turbidity, ph, Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and scale data\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    joblib.dump(scaler, os.path.join(output_dir, 'scaler.pkl'))\n",
    "    print('Data split and scaled successfully.')\n",
    "    print(f\"Scaler saved as '{os.path.join(output_dir, 'scaler.pkl')}'\")\n",
    "except NameError:\n",
    "    print('Error: X or y not defined. Ensure the previous cell ran successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        print(f'{name} trained successfully.')\n",
    "    except Exception as e:\n",
    "        print(f'Error training {name}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "results = {}\n",
    "try:\n",
    "    for name, model in models.items():\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        sensitivity = recall\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        \n",
    "        results[name] = {\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1-Score': f1,\n",
    "            'Sensitivity': sensitivity,\n",
    "            'Specificity': specificity,\n",
    "            'Confusion Matrix': cm.tolist(),\n",
    "            'Model': model\n",
    "        }\n",
    "        \n",
    "        print(f'\\n{name}:')\n",
    "        print(f'Precision: {precision:.4f}')\n",
    "        print(f'Recall: {recall:.4f}')\n",
    "        print(f'Accuracy: {accuracy:.4f}')\n",
    "        print(f'F1-Score: {f1:.4f}')\n",
    "        print(f'Sensitivity: {sensitivity:.4f}')\n",
    "        print(f'Specificity: {specificity:.4f}')\n",
    "        print(f'Confusion Matrix:\\n{cm}')\n",
    "    print('\\nResults dictionary:', results)\n",
    "except Exception as e:\n",
    "    print(f'Error evaluating models: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "try:\n",
    "    best_model_name = max(results, key=lambda x: results[x]['F1-Score'])\n",
    "    best_model = results[best_model_name]['Model']\n",
    "    joblib.dump(best_model, os.path.join(output_dir, 'best_water_quality_model.pkl'))\n",
    "    print(f'Best model ({best_model_name}) saved as {os.path.join(output_dir, \"best_water_quality_model.pkl\")}')\n",
    "except ValueError:\n",
    "    print('Error: No models evaluated. Ensure the previous cell ran successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrices\n",
    "try:\n",
    "    if not results:\n",
    "        raise ValueError('Results accounting_results dictionary is empty. Ensure models were evaluated successfully.')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle('Confusion Matrices for Water Quality Classification', fontsize=16)\n",
    "    axes = axes.ravel()\n",
    "    for idx, (name, metrics) in enumerate(results.items()):\n",
    "        sns.heatmap(metrics['Confusion Matrix'], annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                    cbar=False, annot_kws={'size': 12})\n",
    "        axes[idx].set_title(name)\n",
    "        axes[idx].set_xlabel('Predicted')\n",
    "        axes[idx].set_ylabel('Actual')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    # Save to PNG file\n",
    "    cm_output_path = os.path.join(output_dir, 'confusion_matrices.png')\n",
    "    plt.savefig(cm_output_path, bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    # Embed plot in notebook JSON output as base64\n",
    "    buffer = io.BytesIO()\n",
    "    plt.savefig(buffer, format='png', bbox_inches='tight', dpi=300)\n",
    "    buffer.seek(0)\n",
    "    cm_img_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "    buffer.close()\n",
    "    plt.close()\n",
    "    display(Image(data=base64.b64decode(cm_img_base64)))\n",
    "    print(f'Confusion matrices plot saved as {cm_output_path} and embedded in notebook output.')\n",
    "except Exception as e:\n",
    "    print(f'Error generating confusion matrices plot: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "try:\n",
    "    if not results:\n",
    "        raise ValueError('Results dictionary is empty. Ensure models were evaluated successfully.')\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Model': [name for name in results.keys() for _ in range(6)],\n",
    "        'Metric': ['Precision', 'Recall', 'Accuracy', 'F1-Score', 'Sensitivity', 'Specificity'] * len(results),\n",
    "        'Value': [results[name][metric] for name in results for metric in ['Precision', 'Recall', 'Accuracy', 'F1-Score', 'Sensitivity', 'Specificity']]\n",
    "    })\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='Metric', y='Value', hue='Model', data=metrics_df)\n",
    "    plt.title('Model Performance Comparison', fontsize=16)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save to PNG file\n",
    "    mc_output_path = os.path.join(output_dir, 'model_comparison.png')\n",
    "    plt.savefig(mc_output_path, bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    # Embed plot in notebook JSON output as base64\n",
    "    buffer = io.BytesIO()\n",
    "    plt.savefig(buffer, format='png', bbox_inches='tight', dpi=300)\n",
    "    buffer.seek(0)\n",
    "    mc_img_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "    buffer.close()\n",
    "    plt.close()\n",
    "    display(Image(data=base64.b64decode(mc_img_base64)))\n",
    "    print(f'Model comparison plot saved as {mc_output_path} and embedded in notebook output.')\n",
    "except Exception as e:\n",
    "    print(f'Error generating model comparison plot: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance for Random Forest\n",
    "try:\n",
    "    if 'Random Forest' not in results:\n",
    "        raise ValueError('Random Forest model not found in results.')\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.suptitle('Feature Importance for Water Quality Classification', fontsize=16)\n",
    "    rf_model = results['Random Forest']['Model']\n",
    "    rf_importance = pd.Series(rf_model.feature_importances_, index=X.columns)\n",
    "    rf_importance.sort_values(ascending=False).plot(kind='bar', color='skyblue')\n",
    "    plt.title('Random Forest Feature Importance')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    # Save to PNG file\n",
    "    fi_output_path = os.path.join(output_dir, 'feature_importance.png')\n",
    "    plt.savefig(fi_output_path, bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    # Embed plot in notebook JSON output as base64\n",
    "    buffer = io.BytesIO()\n",
    "    plt.savefig(buffer, format='png', bbox_inches='tight', dpi=300)\n",
    "    buffer.seek(0)\n",
    "    fi_img_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "    buffer.close()\n",
    "    plt.close()\n",
    "    display(Image(data=base64.b64decode(fi_img_base64)))\n",
    "    print(f'Feature importance plot saved as {fi_output_path} and embedded in notebook output.')\n",
    "except Exception as e:\n",
    "    print(f'Error generating feature importance plot: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
